ملخص خطوات مشروع ETL لبيانات أمازون (حتى الآن):
1. مرحلة التخطيط والبيئة (Infrastructure Setup)
إنشاء المشروع: عملنا Folder المشروع وربطناه بـ GitHub لتوثيق كل خطوة.

بناء "المصنع": استخدمنا Docker Compose لإنشاء حاويتين (Containers):

PostgreSQL: قاعدة البيانات اللي هنخزن فيها (المخزن).

pgAdmin: واجهة التحكم عشان نشوف البيانات بعينينا (غرفة التحكم).

الربط التقني: ربطنا pgAdmin بالـ Postgres وتأكدنا إن "المواسير" واصلة ببعضها.

2. مرحلة تأمين الهوية (Authentication)
Kaggle API: حصلنا على "مفتاح الدخول" من موقع كاجل.

إدارة الأسرار: عرفنا إزاي نستخدم ملفات .env وملف kaggle.json عشان كود البايثون يقدر يثبت هويته للموقع ويسحب الداتا بأمان.

3. مرحلة جلب البيانات (Data Ingestion)
كتابة السكريبت: أنشأنا ملف etl_script.py اللي بيقوم بالآتي:

Extract: بيكلم سيرفرات كاجل، يحمل الملف المضغوط، ويفكه في فولدر المشروع.

Read: بيقرأ ملف الـ CSV باستخدام مكتبة Pandas.

Load: بيفتح اتصال مع Postgres ويرفع الـ 1465 سطر لجدول سميناه stg_amazon_sales.

الحالة الحالية للمشروع:
عندنا قاعدة بيانات شغالة.

عندنا كود بايثون "بضغطة زر واحدة" بيجيب أحدث بيانات من الإنترنت ويصبها في القاعدة.

عندنا البيانات الخام (Raw Data) موجودة في جدول الـ Staging ومستنية "التنظيف".